

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Examples and Tutorials &mdash; embARC Machine Learning Inference Library 1.00 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/style_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Convert Tensor" href="../MLI_helpers/convert_tensor.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> embARC Machine Learning Inference Library
          

          
          </a>

          
            
            
              <div class="version">
                1.00
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../library_model/library_model.html">Library Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MLI_FP_data_format/MLI_FP_data_format.html">MLI Fixed-Point Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MLI_kernels/MLI_kernels.html">MLI Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MLI_helpers/MLI_helpers.html">MLI Helpers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Examples and Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-deployment-tutorial-for-caffe-and-cifar-10">Model Deployment Tutorial for Caffe and CIFAR-10</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#instrument-the-model-to-extract-weights-and-data">Instrument the Model to Extract Weights and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#collect-data-range-statistic-for-each-layer">Collect Data Range Statistic for Each Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#define-q-data-format-for-weights-and-data-for-each-layer">Define Q Data Format for Weights and Data for Each Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantize-weights-according-to-defined-q-format">Quantize Weights According to Defined Q-Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploying-operations">Deploying Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-allocation">Data Allocation</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">embARC Machine Learning Inference Library</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Examples and Tutorials</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/Examples_Tutorials/Examples_Tutorials.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="examples-and-tutorials">
<h1>Examples and Tutorials<a class="headerlink" href="#examples-and-tutorials" title="Permalink to this headline">¶</a></h1>
<div class="section" id="model-deployment-tutorial-for-caffe-and-cifar-10">
<h2>Model Deployment Tutorial for Caffe and CIFAR-10<a class="headerlink" href="#model-deployment-tutorial-for-caffe-and-cifar-10" title="Permalink to this headline">¶</a></h2>
<p>In this chapter, we guide you through the manual model deployment process taking CIFAR-10 application as an example. The example considered is based on Caffe standard tutorial. We are not limited to Caffe and similar approach might be applied to other frameworks. Helper scripts considered here are also available in the repo.</p>
<p>Our assumption is that you are familiar with:</p>
<blockquote>
<div><ul class="simple">
<li><strong>Neural networks basics</strong> - convolutions, tensors, ReLU, and so on</li>
<li><strong>Python</strong> - you can read and understand python code</li>
<li><strong>NumPy</strong> - de-facto standard numerical library for python</li>
<li><strong>C programming language</strong> - MLI API is C level API</li>
<li><strong>Caffe framework basics</strong></li>
</ul>
</div></blockquote>
<p>The proposed development process of MLI-based embedded application is depicted with diagram:</p>
<img alt="MLI-Based Application Development Process" class="align-center" src="../_images/1_depl_process.png" />
<ol class="arabic simple">
<li>Model definition and training in some appropriate framework. Ensure that you consider all limitations of the target platform here including memory restriction and frequency budget.</li>
<li>Model deployment implies construction of tested and verified ML module with a defined interface. It is recommended to wrap the module into file-to-file application for convenient debugging and verification.
MLI CIFAR-10 example is exactly of this “unit-testing” kind of applications.</li>
<li>Integrate this module into the target embedded application code with real data.</li>
</ol>
<p>This tutorial focuses on the second step - model deployment.
Manual deployment consists of two main parts:</p>
<blockquote>
<div><ul class="simple">
<li>Deploying data  - Training implies tuning of model parameters.</li>
<li>Deploying operations - The model consists of not only parameters but also algorithm that uses some basic operations or machine learning primitives.</li>
</ul>
</div></blockquote>
<p>Each step of the CIFAR-10 example above is described in separate section below.</p>
<div class="section" id="instrument-the-model-to-extract-weights-and-data">
<h3>Instrument the Model to Extract Weights and Data<a class="headerlink" href="#instrument-the-model-to-extract-weights-and-data" title="Permalink to this headline">¶</a></h3>
<p>After we successfully pass basic Caffe CIFAR 10 tutorial with minor changes, we obtain the following files for deployment:</p>
<blockquote>
<div><ul class="simple">
<li>cifar10_small.prototxt - textual description of the graph which might be visualized as in figure below.</li>
<li>cifar10_small.caffemodel.h5 - data of trained graph. Not necessary h5 format.</li>
<li>mean file (Optional). Not used in this example. The only mean value for all pixels in all channels was used: 128</li>
</ul>
</div></blockquote>
<img alt="CIFAR-10 Graph Visualization" class="align-center" src="../_images/2_CIFAR10_graph.png" />
<p>To get access to model data, install following modules in the environment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">caffe</span>
<span class="kn">import</span> <span class="nn">lmdb</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</pre></div>
</div>
<p>To transform Caffe representation of data into numpy arrays, load and initialize model by constructing Caffe Net object which takes prototext and caffemodel files as parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">Net</span><span class="p">(</span><span class="s2">&quot;cifar10_small.prototxt&quot;</span><span class="p">,</span> <span class="s2">&quot;cifar10_small.caffemodel.h5&quot;</span><span class="p">,</span> <span class="n">caffe</span><span class="o">.</span><span class="n">TEST</span><span class="p">)</span>

<span class="c1"># Get key to the first output of net (here it&#39;s a prob data)</span>
<span class="n">out_key</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>To deploy the model correctly, analyze the two subsets of data which might be transformed to numpy arrays by native interface:</p>
<ol class="arabic simple">
<li>Model parameters</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">data_obj</span> <span class="ow">in</span> <span class="n">classifier</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
   <span class="n">weights_np_arr</span> <span class="o">=</span> <span class="n">data_obj</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span> <span class="c1"># conv weights is blob #0</span>
   <span class="n">bias_np_arr</span> <span class="o">=</span> <span class="n">data_obj</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>   <span class="c1"># conv bias is blob #1</span>
   <span class="c1"># further operations with data</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>Intermediate results</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">blob_name</span><span class="p">,</span> <span class="n">data_obj</span> <span class="ow">in</span> <span class="n">classifier</span><span class="o">.</span><span class="n">blobs</span><span class="o">.</span><span class="n">iter</span><span class="p">():</span>
    <span class="n">data_np_arr</span> <span class="o">=</span> <span class="n">data_obj</span><span class="o">.</span><span class="n">data</span>
    <span class="c1"># further operations with data</span>
</pre></div>
</div>
<p>In Caffe, these objects are referred as blobs. While model parameters in this case are fixed in the inference time (after model had been trained), intermediate results vary from one model input to another. Therefore, intermediate blob content differs depending on the input you feed into net.</p>
<p>To update classifier object state, use the following command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">forward_all</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)[</span><span class="n">out_key</span><span class="p">]</span>
</pre></div>
</div>
<p>Here:</p>
<blockquote>
<div><ul class="simple">
<li><cite>test_data</cite> is a numpy array with input vector (CIFAR-10 dataset entity)</li>
<li><cite>out_key</cite> is our “key” to the network output we had defined early</li>
<li><cite>pred</cite> is the output.</li>
</ul>
</div></blockquote>
<p>Using defined pieces of Python code, you can extract all the required data from the model and adapt it to an embedded MLI based application.</p>
</div>
<div class="section" id="collect-data-range-statistic-for-each-layer">
<h3>Collect Data Range Statistic for Each Layer<a class="headerlink" href="#collect-data-range-statistic-for-each-layer" title="Permalink to this headline">¶</a></h3>
<p>Quantization process is not only meant to convert weights data to fixed point representation, but also meant to define ranges of all the intermediate data for each layer. For this purpose, run the model on some representative data subset and gather statistics for all intermediate results. It is recommended to use full training subset.</p>
<p>To accomplish this using previously defined instruments, see this sample code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Open dataset and get cursor</span>
<span class="n">lmdb_env</span> <span class="o">=</span> <span class="n">lmdb</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;cifar10_train_lmdb&quot;</span><span class="p">)</span>
<span class="n">lmdb_txn</span> <span class="o">=</span> <span class="n">lmdb_env</span><span class="o">.</span><span class="n">begin</span><span class="p">()</span>
<span class="n">lmdb_cursor</span> <span class="o">=</span> <span class="n">lmdb_txn</span><span class="o">.</span><span class="n">cursor</span><span class="p">()</span>

<span class="c1"># Init data parser and dictionary for min/max statistic</span>
<span class="n">datum</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">proto</span><span class="o">.</span><span class="n">caffe_pb2</span><span class="o">.</span><span class="n">Datum</span><span class="p">()</span>
<span class="n">ir_ranges</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">lmdb_cursor</span><span class="p">:</span>
    <span class="n">datum</span><span class="o">.</span><span class="n">ParseFromString</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="n">data_raw</span> <span class="o">=</span> <span class="n">caffe</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">datum_to_array</span><span class="p">(</span><span class="n">datum</span><span class="p">)</span>

    <span class="c1"># Don&#39;t forget about pre-processing if you need it (Mean and scale)</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([(</span><span class="n">data_raw</span> <span class="o">-</span> <span class="mf">128.0</span><span class="p">)</span><span class="o">/</span><span class="mf">128.0</span><span class="p">])</span>
    <span class="n">test_label</span> <span class="o">=</span> <span class="n">datum</span><span class="o">.</span><span class="n">label</span>

    <span class="c1"># Model Inference on loaded data</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">forward_all</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)[</span><span class="n">out_key</span><span class="p">]</span>

    <span class="c1"># Update ranges (Note: dictionary requires proper initialization in first pass)</span>
    <span class="k">for</span> <span class="n">blob_name</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">classifier</span><span class="o">.</span><span class="n">blobs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">ir_ranges</span><span class="p">[</span><span class="n">blob_name</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">ir_ranges</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
        <span class="n">ir_ranges</span><span class="p">[</span><span class="n">blob_name</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">ir_ranges</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
</pre></div>
</div>
<p>For simplicity, only maximum/minimum range of our data is collected. However, you can choose a more sophisticated approach which also may affect the choice for calibration data.
A similar range definition is required for model parameters. As weights are fixed after training and are not changed in inference time, you can just transform data to numpy arrays. It provides min() and max() methods for easy range definition. It also keeps the shape of data we need for MLI tensor definition later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">weights_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">bias_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">layer_name</span><span class="p">,</span> <span class="n">data_obj</span> <span class="ow">in</span> <span class="n">classifier</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">weights_np_dict</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_obj</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
    <span class="n">bias_np_dict</span> <span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_obj</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
<div class="admonition attention">
<p class="first admonition-title">Attention</p>
<p class="last">Using batch normalization and scale layers with convolution requires integratation of the parameters of these layers into weights and biases through manual recalculation of weights.
This is not straightforward and is beyond the scope of this tutorial.</p>
</div>
</div>
<div class="section" id="define-q-data-format-for-weights-and-data-for-each-layer">
<h3>Define Q Data Format for Weights and Data for Each Layer<a class="headerlink" href="#define-q-data-format-for-weights-and-data-for-each-layer" title="Permalink to this headline">¶</a></h3>
<p>MLI supports fixed point format defined by Q-notation (see <a class="reference internal" href="../MLI_FP_data_format/MLI_FP_data_format.html#mli-fpd-fmt"><span class="std std-ref">MLI Fixed-Point Data Format</span></a> section). The next step is to find the appropriate Q-format of input, output and coefficients for each layer to correctly represent float values. This format is fixed in inference time (at least for constant weights). We define the number of integer bits and fractional bits can be easily derived from it. The following table specifies the derivation of integer bits from CIFAR-10 model statistics:</p>
<table border="1" class="colwidths-auto docutils" id="id1">
<caption><span class="caption-text">Integer Bits Derivation form CIFAR-10 Model Statistics</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<thead valign="bottom">
<tr class="row-odd"><th class="head" rowspan="2"><strong>CIFAR10</strong></th>
<th class="head" colspan="4"><strong>Maximum abs values of tensors</strong></th>
<th class="head" colspan="4"><strong>Minimum Integer bits requirements</strong></th>
</tr>
<tr class="row-even"><th class="head">Layer Input
Max ABS Value</th>
<th class="head">Layer Weights
Max ABS Value</th>
<th class="head">Layer Bias
Max ABS Value</th>
<th class="head">Layer Out
Max ABS Value</th>
<th class="head">Layer Input
Bits</th>
<th class="head">Layer Weights
Bits</th>
<th class="head">Layer bias
Bits</th>
<th class="head">Layer out
Bits</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-odd"><td>Layer 1_conv</td>
<td>0.99</td>
<td>0.49</td>
<td>0.73</td>
<td>7.03</td>
<td>0</td>
<td>-1</td>
<td>0</td>
<td>3</td>
</tr>
<tr class="row-even"><td>Layer 2_conv</td>
<td>7.03</td>
<td>0.35</td>
<td>0.39</td>
<td>21.88</td>
<td>3</td>
<td>-1</td>
<td>-1</td>
<td>5</td>
</tr>
<tr class="row-odd"><td>Layer 3_conv</td>
<td>17.89</td>
<td>0.29</td>
<td>0.18</td>
<td>27.22</td>
<td>5</td>
<td>-1</td>
<td>-2</td>
<td>5</td>
</tr>
<tr class="row-even"><td>Layer 4_fc</td>
<td>22.14</td>
<td>0.41</td>
<td>0.2</td>
<td>20.798</td>
<td>5</td>
<td>-1</td>
<td>-2</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>On the left part of the table are the absolute maximum of ranges for all tensors we had defined early:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_abs_val</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">val_max</span><span class="p">),</span> <span class="nb">abs</span><span class="p">(</span><span class="n">val_min</span><span class="p">))</span>
</pre></div>
</div>
<p>On the right are the calculated minimum number of integer bits:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">int_bits</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">max_abs_val</span><span class="p">)))</span>
</pre></div>
</div>
<p>Fractional bits are calculated as container size minus integer bits.</p>
<p>For 8-bit depth of data, this is sufficient, but for 16-bit minor corrections are required. MLI uses 40bit accumulator which provides 9 extra bits for processing up to 512 MAC operations in a row on 16x16 operands. For longer MAC series, keep some bits in the operands unused to guarantee that the result does not saturate in accumulation (for more info see <a class="reference internal" href="../MLI_FP_data_format/MLI_FP_data_format.html#quant-acc-bit-depth"><span class="std std-ref">Quantization: Influence of Accumulator Bit Depth</span></a> ).</p>
<p>Consider a small example not directly related to the CIFAR-10:</p>
<table border="1" class="colwidths-given docutils align-center" id="id2">
<caption><span class="caption-text">Integer Bits Derivation Considering Accumulator Restrictions</span><a class="headerlink" href="#id2" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="30%" />
<col width="30%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>&#160;</td>
<td>&#160;</td>
<td><strong>Layer X Conv</strong></td>
<td><strong>Layer X+1 FC</strong></td>
</tr>
<tr class="row-even"><td rowspan="3"><strong>Integer Bit Requirements</strong>
<em>(fx8 operands)</em></td>
<td>Layer Input Bits</td>
<td>5</td>
<td>5</td>
</tr>
<tr class="row-odd"><td>Layer Weight Bits</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr class="row-even"><td>Layer Output Bits</td>
<td>5</td>
<td>5</td>
</tr>
<tr class="row-odd"><td rowspan="3"><strong>Accumulator Restrictions</strong></td>
<td>MACs per Output value</td>
<td>801 (32*5*5+1)</td>
<td>1025 (64*16+1)</td>
</tr>
<tr class="row-even"><td>Required Extra Bits</td>
<td>10</td>
<td>11</td>
</tr>
<tr class="row-odd"><td>Not Enough Bits</td>
<td>10 - 9 = 1</td>
<td>11 - 9 = 2</td>
</tr>
<tr class="row-even"><td rowspan="3"><strong>Integer Bit Requirements</strong>
<em>(fx16 operands)</em></td>
<td>Layer Input Bits
(updated)</td>
<td>5 + 1 = 6</td>
<td>5 + 1 = 6</td>
</tr>
<tr class="row-odd"><td>Layer Weight Bits
(updated)</td>
<td>-1</td>
<td>-1 + 1 = 0</td>
</tr>
<tr class="row-even"><td>Layer Output bits
(updated)</td>
<td>6 (next layer in)</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>For convolution layer X, number of integer bits are defined as before. And for each output value, the following number of sequential accumulations is required: 32[number of channels] * (5*5) [kernel size] +1 [bias] = 801 operations. 10 extra bits are required for accumulation while only 9 are available. For this reason, the number of integer bits for layer input are increased.</p>
<p>For the following fully connected layer, 11 extra bits are required and 2 bits need to be distributed. It’s recommended to do this evenly between operands. Note that number of convolution’s output fractional bits also needs to be changed to be aligned with next fully connected input.</p>
<p>For 8-bit operands,you do not need to perform this adjustment unless your MAC series is more than 131072 operations in which case, apply similar approach. After considering accumulator restrictions for CIFAR-10 example with 16-bit operands, you get the following table:</p>
<table border="1" class="colwidths-given docutils align-center" id="id3">
<caption><span class="caption-text">Integer Bits Derivation from CIFAR-10 Model Statistics Considering Accumulator Restrictions</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="21%" />
<col width="21%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>&#160;</td>
<td>&#160;</td>
<td><strong>Layer 1_conv</strong></td>
<td><strong>Layer 2_conv</strong></td>
<td><strong>Layer 3_conv</strong></td>
<td><strong>Layer 4_fc</strong></td>
</tr>
<tr class="row-even"><td rowspan="3"><strong>Integer Bit Requirements</strong>
<em>(fx8 operands)</em></td>
<td>Layer Input Bits</td>
<td>0</td>
<td>3</td>
<td>5</td>
<td>5</td>
</tr>
<tr class="row-odd"><td>Layer Weight Bits</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr class="row-even"><td>Layer Output Bits</td>
<td>3</td>
<td>5</td>
<td>5</td>
<td>5</td>
</tr>
<tr class="row-odd"><td rowspan="3"><strong>Accumulator Restrictions</strong></td>
<td>MACs per Output Value</td>
<td>76 (3*5*5+1)</td>
<td>801 (64*16+1)</td>
<td>401 (16*5*5+1)</td>
<td>513 (32*16+1)</td>
</tr>
<tr class="row-even"><td>Required Extra Bits</td>
<td>7</td>
<td>10</td>
<td>9</td>
<td>10</td>
</tr>
<tr class="row-odd"><td>Not Enough Bits</td>
<td>0</td>
<td>10 - 9 = 1</td>
<td>9 - 9 = 0</td>
<td>10 - 9 = 1</td>
</tr>
<tr class="row-even"><td rowspan="3"><strong>Integer Bit Requirements</strong>
<em>(fx16 operands)</em></td>
<td>Layer Input Bits
(updated)</td>
<td>0</td>
<td>3 + 1 = 4</td>
<td>5</td>
<td>5 + 1 = 6</td>
</tr>
<tr class="row-odd"><td>Layer Weight Bits
(updated)</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr class="row-even"><td>Layer Output Bits
(updated)</td>
<td>4 (next layer in)</td>
<td>5</td>
<td>6 (next layer in)</td>
<td>5</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Defining Q format in this way, you can guarantee that accumulator is not saturated while a single output is being calculated. But the restriction may be loosened if you are sure about your data. For example, look at the final fully connected layer above: 9 bits (512 MACs) are enough if we do not consider bias addition. Analyze how likely is it that for 1 extra addition result will overflow the defined range. Moreover, saturation of results might have a minor effect on the network accuracy.</p>
</div>
</div>
<div class="section" id="quantize-weights-according-to-defined-q-format">
<h3>Quantize Weights According to Defined Q-Format<a class="headerlink" href="#quantize-weights-according-to-defined-q-format" title="Permalink to this headline">¶</a></h3>
<p>After extracting coefficients in numpy array objects and defining Qm.n format for data, define MLI structures for kernels and export the quantized data.</p>
<p>Consider a static allocation of data. To extract weights, you may make pre-processor quantize data for you in compile-time by wrapping each coefficient into macro function. It is slower and uses more memory resources of your machine for compilation, but it is worth if the model is not so big.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#define QMN(type, fraq, val)   \</span>
<span class="cp">     (type)(val * (1u &lt;&lt; (fraq)) + ((val &gt;= 0)? 0.5f: -0.5f))</span>
<span class="cp">#define L1_WQ(val)   QMN(int8_t,  8, val)</span>
<span class="cp">#define L1_BQ(val)   QMN(int8_t,  7, val)</span>
<span class="k">const</span>  <span class="kt">int8_t</span> <span class="n">L1_conv_wt_buf</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>\
     <span class="n">L1_WQ</span><span class="p">(</span> <span class="mf">0.096343018</span><span class="p">),</span><span class="n">L1_WQ</span><span class="p">(</span> <span class="mf">0.148116693</span><span class="p">),</span><span class="n">L1_WQ</span><span class="p">(</span> <span class="mf">0.023189211</span><span class="p">),</span> <span class="p">...</span> \
     <span class="n">L1_WQ</span><span class="p">(</span><span class="o">-</span><span class="mf">0.123411559</span><span class="p">),</span><span class="n">L1_WQ</span><span class="p">(</span><span class="o">-</span><span class="mf">0.047247209</span><span class="p">),</span><span class="n">L1_WQ</span><span class="p">(</span> <span class="mf">0.091348067</span><span class="p">),</span> <span class="p">...</span> \
     <span class="p">...</span>
<span class="p">};</span>
<span class="k">const</span> <span class="kt">int8_t</span>  <span class="n">L1_conv_bias_buf</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>\
     <span class="n">L1_BQ</span><span class="p">(</span> <span class="mf">0.058115590</span><span class="p">),</span><span class="n">L1_BQ</span><span class="p">(</span><span class="o">-</span><span class="mf">0.098249219</span><span class="p">),</span><span class="n">L1_BQ</span><span class="p">(</span> <span class="mf">0.456347317</span><span class="p">),</span> <span class="p">...</span> \
     <span class="n">L1_BQ</span><span class="p">(</span><span class="o">-</span><span class="mf">0.135683402</span><span class="p">),</span><span class="n">L1_BQ</span><span class="p">(</span><span class="o">-</span><span class="mf">0.039959636</span><span class="p">),</span><span class="n">L1_BQ</span><span class="p">(</span> <span class="mf">0.527986348</span><span class="p">),</span> <span class="p">...</span> \
     <span class="p">...</span>
<span class="p">};</span>
</pre></div>
</div>
<p>Alternatively, you can quantize data externally in the same way and just put it into code.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">const</span> <span class="kt">int8_t</span> <span class="n">L1_conv_wt_buf</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">25</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">12</span><span class="p">,</span> <span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="p">...}</span>
<span class="k">const</span> <span class="kt">int8_t</span> <span class="n">L1_conv_bt_buf</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">12</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="p">...}</span>
</pre></div>
</div>
<p>To describe raw data by tensor structures, see this sample code:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Conv 1 Layer weights and biases tensors</span>
<span class="k">static</span> <span class="k">const</span> <span class="n">mli_tensor</span> <span class="n">L1_conv_wt</span> <span class="o">=</span> <span class="p">{</span>
     <span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="n">L1_conv_wt_buf</span><span class="p">,</span>
     <span class="p">.</span><span class="n">capacity</span> <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">L1_conv_wt_buf</span><span class="p">),</span>
     <span class="p">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">{</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">},</span>           <span class="c1">// Get Shape from the NP Array</span>
     <span class="p">.</span><span class="n">rank</span> <span class="o">=</span>  <span class="mi">4</span><span class="p">,</span>
     <span class="p">.</span><span class="n">el_type</span> <span class="o">=</span> <span class="n">MLI_EL_FX_8</span><span class="p">,</span>
     <span class="p">.</span><span class="n">el_params</span><span class="p">.</span><span class="n">fx</span><span class="p">.</span><span class="n">frac_bits</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
 <span class="p">};</span>
<span class="k">static</span> <span class="k">const</span> <span class="n">mli_tensor</span> <span class="n">L1_conv_bias</span> <span class="o">=</span> <span class="p">{</span>
     <span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span><span class="n">L1_conv_bias_buf</span><span class="p">,</span>
     <span class="p">.</span><span class="n">capacity</span> <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">L1_conv_bias_buf</span><span class="p">),</span>
     <span class="p">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">{</span><span class="mi">32</span><span class="p">},</span>
     <span class="p">.</span><span class="n">rank</span> <span class="o">=</span>  <span class="mi">1</span><span class="p">,</span>
     <span class="p">.</span><span class="n">el_type</span> <span class="o">=</span> <span class="n">MLI_EL_FX_8</span><span class="p">,</span>
     <span class="p">.</span><span class="n">el_params</span><span class="p">.</span><span class="n">fx</span><span class="p">.</span><span class="n">frac_bits</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span>
 <span class="p">};</span>
<span class="c1">// Next value will be passed with output tensor structure</span>
<span class="cp">#define CONV1_OUT_FRAQ_BITS (4)</span>
<span class="p">...</span>
</pre></div>
</div>
<p>Extract the shape of the data and its rank (number of dimensions) from numpy object. Set the container parameters, including its type and number of fractional bits, according to bit depth that you want to use and integer bits defined earlier. For MAC-based kernels, allocate the number of fractional bits as well for output (<cite>CONV1_OUT_FRAQ_BITS</cite>).</p>
</div>
<div class="section" id="deploying-operations">
<h3>Deploying Operations<a class="headerlink" href="#deploying-operations" title="Permalink to this headline">¶</a></h3>
<p>To define MLI operations and its parameters for trained graph, start from input data as shown in the figure below.</p>
<img alt="CIFAR-10 Graph Visualization: Input Data" class="align-center" src="../_images/3_op_map_step1.png" />
<p>Assume that the input is an RGB image in HWC layout, while MLI mostly optimized for CHW layout
(see MLI documentation section <a class="reference internal" href="../library_model/data.html#data-muldim"><span class="std std-ref">Data Layout of Multidimensional Feature Maps</span></a>).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Layout is not only about input of network, but also related to intermediate results. Primitive optimization techniques differ for different layouts.</p>
</div>
<p>Transpose data by permute layer with appropriate parameters:</p>
<table border="1" class="colwidths-given docutils" id="id4">
<caption><span class="caption-text">Example of Permute Layer for Different Layout Consideration</span><a class="headerlink" href="#id4" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>ProtoText description</strong></td>
<td><div class="code first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;Input&quot;</span>
  <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span>
  <span class="n">input_param</span> <span class="p">{</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span> <span class="n">dim</span><span class="p">:</span> <span class="mi">1</span>
  <span class="n">dim</span><span class="p">:</span> <span class="mi">3</span> <span class="n">dim</span><span class="p">:</span> <span class="mi">32</span> <span class="n">dim</span><span class="p">:</span> <span class="mi">32</span> <span class="p">}</span> <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>MLI Function</strong></td>
<td><div class="first last highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mli_status</span> <span class="nf">mli_krn_permute_fx8</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">in</span><span class="p">,</span>        <span class="c1">// Input tensor</span>
    <span class="k">const</span> <span class="n">mli_permute_cfg</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">,</span>  <span class="c1">// Permute configuration</span>
    <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">out</span>              <span class="c1">// Output tensor</span>
  <span class="p">);</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><div class="first last highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mli_permute_cfg</span> <span class="n">permute_hwc2chw_cfg</span> <span class="o">=</span> <span class="p">{</span>
   <span class="p">.</span><span class="n">perm_dim</span> <span class="o">=</span>
       <span class="p">{</span><span class="n">FMAP_C_DIM_HWC</span><span class="p">,</span> <span class="c1">// 2</span>
        <span class="n">FMAP_H_DIM_HWC</span><span class="p">,</span> <span class="c1">// 0</span>
        <span class="n">FMAP_W_DIM_HWC</span><span class="p">}</span> <span class="c1">// 1</span>
<span class="p">};</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p>Next, consider convolution and ReLU layers:</p>
<img alt="CIFAR-10 Graph Visualization: Output Data" class="align-center" src="../_images/4_op_map_step2.png" />
<p>Parameters of all convolutions in the model are the same, so you may use the only function for all of them, which is specialized for exactly these parameters. Additionally, MLI convolutions may perform ReLU transformation while saving the result. Hence, there is no need to use separate function (even if it is possible to do so). The only exception is the first layer, where maxpooling is between ReLU and convolution. Luckily, it is a maxpooling operation, not an average pooling one. In this case, you may do ReLU first, and max pooling after without any effect in inference:</p>
<table border="1" class="colwidths-given docutils" id="id5">
<caption><span class="caption-text">Example of 2D-Convolution Layer with Embedded ReLU</span><a class="headerlink" href="#id5" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>ProtoText description</strong></td>
<td><div class="code first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;conv2&quot;</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;Convolution&quot;</span>
  <span class="n">bottom</span><span class="p">:</span> <span class="s2">&quot;pool1&quot;</span>
  <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;conv2&quot;</span>
  <span class="n">convolution_param</span> <span class="p">{</span>
     <span class="n">num_output</span><span class="p">:</span> <span class="mi">32</span> <span class="n">pad</span><span class="p">:</span> <span class="mi">2</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">5</span> <span class="n">stride</span><span class="p">:</span> <span class="mi">1</span>  <span class="p">}}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;relu2&quot;</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;ReLU&quot;</span>
  <span class="n">bottom</span><span class="p">:</span> <span class="s2">&quot;conv2&quot;</span>
  <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;conv2&quot;</span><span class="p">}</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>MLI Function</strong></td>
<td><div class="first last highlight-c++ notranslate"><div class="highlight"><pre><span></span> <span class="n">mli_status</span> <span class="nf">mli_krn_conv2d_chw_fx8_k5x5_str1_krnpad</span><span class="p">(</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">in</span><span class="p">,</span>       <span class="c1">// Input tensor</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">weights</span><span class="p">,</span>  <span class="c1">// Weights tensor</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">bias</span><span class="p">,</span>     <span class="c1">// Biases tensor</span>
   <span class="k">const</span> <span class="n">mli_conv2d_cfg</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">,</span>  <span class="c1">// Convolution config</span>
   <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">out</span>             <span class="c1">// Output tensor</span>
<span class="p">);</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><strong>MLI Function Config</strong></td>
<td><div class="first last highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mli_conv2d_cfg</span> <span class="n">shared_conv_cfg</span> <span class="o">=</span> <span class="p">{</span>
  <span class="p">.</span><span class="n">stride_height</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="n">stride_width</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
  <span class="p">.</span><span class="n">padding_bottom</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="n">padding_top</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
  <span class="p">.</span><span class="n">padding_left</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="n">padding_right</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
  <span class="p">.</span><span class="n">relu</span><span class="p">.</span><span class="n">type</span> <span class="o">=</span> <span class="n">MLI_RELU_GEN</span>
 <span class="p">};</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<img alt="CIFAR-10 Graph Visualization: Pooling Layers" class="align-center" src="../_images/5_op_map_step3.png" />
<p>MLI Pooling behavior differs from Caffe default behavior. In Caffe, padding is implied for some combinations of layer parameters, even if not specified. You should indicate padding clearly because it is meant in the Caffe. It was done for compatibility with other frameworks.</p>
<table border="1" class="colwidths-given docutils" id="id6">
<caption><span class="caption-text">Example Pooling Layer with Padding</span><a class="headerlink" href="#id6" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>ProtoText description</strong></td>
<td><div class="code first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;pool1&quot;</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;Pooling&quot;</span>
  <span class="n">bottom</span><span class="p">:</span> <span class="s2">&quot;conv1&quot;</span>
  <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;pool1&quot;</span>
  <span class="n">pooling_param</span> <span class="p">{</span>
       <span class="n">pool</span><span class="p">:</span> <span class="n">MAX</span>    <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">3</span>    <span class="n">stride</span><span class="p">:</span> <span class="mi">2</span>  <span class="p">}}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;pool2&quot;</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;Pooling&quot;</span>
  <span class="n">bottom</span><span class="p">:</span> <span class="s2">&quot;conv2&quot;</span>
  <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;pool2&quot;</span>
  <span class="n">pooling_param</span> <span class="p">{</span>
       <span class="n">pool</span><span class="p">:</span> <span class="n">AVE</span>    <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">3</span>    <span class="n">stride</span><span class="p">:</span> <span class="mi">2</span>  <span class="p">}}</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>MLI Function</strong></td>
<td><div class="first last highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mli_status</span> <span class="nf">mli_krn_maxpool_chw_fx8_k3x3</span><span class="p">(</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">in</span><span class="p">,</span>     <span class="c1">// Input tensor</span>
   <span class="k">const</span> <span class="n">mli_pool_cfg</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">,</span>  <span class="c1">// Pooling configuration</span>
   <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">out</span>           <span class="c1">// Output tensor</span>
<span class="p">);</span>
<span class="n">mli_status</span> <span class="nf">mli_krn_avepool_chw_fx8_k3x3</span><span class="p">(</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">in</span><span class="p">,</span>     <span class="c1">// Input tensor</span>
   <span class="k">const</span> <span class="n">mli_pool_cfg</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">,</span>  <span class="c1">// Pooling configuration</span>
   <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">out</span>           <span class="c1">// Output tensor</span>
<span class="p">);</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><strong>MLI Function Config</strong></td>
<td><div class="first last highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mli_pool_cfg</span> <span class="n">shared_pool_cfg</span> <span class="o">=</span> <span class="p">{</span>
   <span class="p">.</span><span class="n">kernel_height</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="p">.</span><span class="n">kernel_width</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
   <span class="p">.</span><span class="n">stride_height</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="n">stride_width</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
   <span class="p">.</span><span class="n">padding_bottom</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="n">padding_top</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
   <span class="p">.</span><span class="n">padding_left</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">.</span><span class="n">padding_right</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">};</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All specializations for pooling and convolution group of primitives are declared in the appropriate header files (see it in the GitHub repository). Functions without specialization postfix work like switchers, analyzing parameters and choosing proper one to delegate actual job. This can be used in debug to define a proper specialization.</p>
</div>
<p>Consider the last two operations:</p>
<img alt="CIFAR-10 Graph Visualization: Final Layers" class="align-center" src="../_images/6_op_map_step4.png" />
<p>Fully connected (referred as Inner Product in Caffe) and softmax don’t require any specific analysis.</p>
<table border="1" class="colwidths-given docutils" id="id7">
<caption><span class="caption-text">Example of Function Choosing Optimal Specialization</span><a class="headerlink" href="#id7" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>ProtoText description</strong></td>
<td><div class="code first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;ip1&quot;</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;InnerProduct&quot;</span>
  <span class="n">bottom</span><span class="p">:</span> <span class="s2">&quot;pool3&quot;</span>
  <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;ip1&quot;</span>
  <span class="n">inner_product_param</span> <span class="p">{</span>  <span class="n">num_output</span><span class="p">:</span> <span class="mi">10</span>  <span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;prob&quot;</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;Softmax&quot;</span>
  <span class="n">bottom</span><span class="p">:</span> <span class="s2">&quot;ip1&quot;</span>
  <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;prob&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>MLI Function</strong></td>
<td><div class="first last highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mli_status</span> <span class="nf">mli_krn_fully_connected_fx8</span><span class="p">(</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">in</span><span class="p">,</span>      <span class="c1">// Input tensor</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">weights</span><span class="p">,</span> <span class="c1">// Weights tensor</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">bias</span><span class="p">,</span>    <span class="c1">// Bias tensor</span>
   <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">out</span>            <span class="c1">// Output tensor</span>
<span class="p">);</span>
<span class="n">mli_status</span> <span class="nf">mli_krn_softmax_fx8</span><span class="p">(</span>
   <span class="k">const</span> <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">in</span><span class="p">,</span>   <span class="c1">// Input tensor</span>
   <span class="n">mli_tensor</span> <span class="o">*</span> <span class="n">out</span>         <span class="c1">// Output tensor</span>
<span class="p">);</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-odd"><td><strong>MLI Function Config</strong></td>
<td>No configuration is required. Tensors provide all necessary
information</td>
</tr>
</tbody>
</table>
<p>When data extracted properly (wrapped into tensors and configuration structures), and functions for inference are defined, execution sequence in terms of MLI calls look like this:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// LAYER 0: Change RGB Image layout</span>
<span class="n">mli_krn_permute_fx16</span><span class="p">(</span><span class="o">&amp;</span><span class="n">input</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">permute_hwc2chw_cfg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">);</span>

<span class="c1">// LAYER 1</span>
<span class="n">ir_X</span><span class="p">.</span><span class="n">el_params</span><span class="p">.</span><span class="n">fx</span><span class="p">.</span><span class="n">frac_bits</span> <span class="o">=</span> <span class="n">CONV1_OUT_FRAQ</span><span class="p">;</span>
<span class="n">mli_krn_conv2d_chw_fx8_k5x5_str1_krnpad</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L1_conv_wt</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L1_conv_bias</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">shared_conv_cfg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_X</span><span class="p">);</span>
<span class="n">mli_krn_maxpool_chw_fx16_k3x3</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_X</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">shared_pool_cfg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">);</span>

<span class="c1">// LAYER 2</span>
<span class="n">ir_X</span><span class="p">.</span><span class="n">el_params</span><span class="p">.</span><span class="n">fx</span><span class="p">.</span><span class="n">frac_bits</span> <span class="o">=</span> <span class="n">CONV2_OUT_FRAQ</span><span class="p">;</span>
<span class="n">mli_krn_conv2d_chw_fx8_k5x5_str1_krnpad</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L2_conv_wt</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L2_conv_bias</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">shared_conv_cfg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_X</span><span class="p">);</span>
<span class="n">mli_krn_avepool_chw_fx16_k3x3_krnpad</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_X</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">shared_pool_cfg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">);</span>

<span class="c1">// LAYER 3</span>
<span class="n">ir_X</span><span class="p">.</span><span class="n">el_params</span><span class="p">.</span><span class="n">fx</span><span class="p">.</span><span class="n">frac_bits</span> <span class="o">=</span> <span class="n">CONV3_OUT_FRAQ</span><span class="p">;</span>
<span class="n">mli_krn_conv2d_chw_fx8_k5x5_str1_krnpad</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L3_conv_wt</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L3_conv_bias</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">shared_conv_cfg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_X</span><span class="p">);</span>
<span class="n">mli_krn_avepool_chw_fx16_k3x3_krnpad</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_X</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">shared_pool_cfg</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">);</span>

<span class="c1">// LAYER 4</span>
<span class="n">ir_X</span><span class="p">.</span><span class="n">el_params</span><span class="p">.</span><span class="n">fx</span><span class="p">.</span><span class="n">frac_bits</span> <span class="o">=</span> <span class="n">FC4_OUT_FRAQ</span><span class="p">;</span>
<span class="n">mli_krn_fully_connected_fx16</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_Y</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L4_fc_wt</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">L4_fc_bias</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ir_X</span><span class="p">);</span>
<span class="n">mli_krn_softmax_fx16</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ir_X</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">output</span><span class="p">);</span>
</pre></div>
</div>
<p>Here, you can see the IR tensors for storing intermediate results (ir_X and ir_Y). They are used in double-buffering style. Each primitive uses only buffers pointed by tensors. Fill the rest of the fields of tensors to provide a valid value to next primitive as input. Hence, before using, output tensor must keep only pointer to buffer and its capacity + number of fractional bits for MAC based operations.</p>
</div>
<div class="section" id="data-allocation">
<h3>Data Allocation<a class="headerlink" href="#data-allocation" title="Permalink to this headline">¶</a></h3>
<p>To estimate how much memory is required, and decide where to keep the operands in the address space, consider EM9D based target with AGU and XY memory. Keeping operands in a different memory banks (DCCM, XCCM, YCCM) significantly increases performance. Ensure that you organize data flow properly for this work properly.</p>
<p>Here is one of the options:</p>
<img alt="Data Flow Organization Example Considering Data Layout" class="align-center" src="../_images/7_data_allocation.png" />
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can use two memories instead of three without effect on XY performance.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../MLI_helpers/convert_tensor.html" class="btn btn-neutral float-left" title="Convert Tensor" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Synopsys, Inc

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>