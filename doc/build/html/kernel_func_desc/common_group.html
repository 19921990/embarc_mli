

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Common Group &mdash; embARC Machine Learning Inference Library 1.00a documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/style_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Transform Group" href="transform_group.html" />
    <link rel="prev" title="Pooling Group" href="pooling_group.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> embARC Machine Learning Inference Library
          

          
          </a>

          
            
            
              <div class="version">
                1.00a
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../initial/index.html">Definitions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../initial/index.html#revision-history">Revision history</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mli_api_desc/index.html">Machine Learning Inference Library API</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Kernel Functions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="convolution_group.html">Convolution Group</a></li>
<li class="toctree-l2"><a class="reference internal" href="pooling_group.html">Pooling Group</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Common Group</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fully-connected">Fully Connected</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#function-configuration-structure">Function Configuration Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#api">API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kernel-specializations">Kernel Specializations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conditions-for-applying-the-kernel">Conditions for Applying the Kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#long-short-term-memory-lstm-cell">Long Short Term Memory (LSTM) Cell</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#fn-conf-lstm">Function Configuration Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#api-lstm">API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kernel-specializations-1">Kernel Specializations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cond-lstm">Conditions for Applying the Kernel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#basic-rnn-cell">Basic RNN Cell</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#fn-conf-brnn">Function Configuration Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#api-brnn">API</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kernel-specializations-2">Kernel Specializations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conditions-for-applying-the-kernel-2">Conditions for Applying the Kernel</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="transform_group.html">Transform Group</a></li>
<li class="toctree-l2"><a class="reference internal" href="element_wise_group.html">Elementwise Group</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_manipulation_group.html">Data Manipulation Group</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../helper_func_desc/index.html">Helper Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../final/references.html">References</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">embARC Machine Learning Inference Library</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Kernel Functions</a> &raquo;</li>
        
      <li>Common Group</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/kernel_func_desc/common_group.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="common-group">
<h1>Common Group<a class="headerlink" href="#common-group" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div>Common group provides operations for common machine learning
processing and other mathematical and statistic calculations.</div></blockquote>
<div class="section" id="fully-connected">
<h2>Fully Connected<a class="headerlink" href="#fully-connected" title="Permalink to this headline">¶</a></h2>
<div class="figure" id="id9">
<span id="f-fully-connected"></span><img alt="../_images/image110.png" src="../_images/image110.png" />
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Fully Connected Layer Schematic Representation</span></p>
<div class="legend">
<p>This kernel implements fully connected layer, also usually referred
to as the inner product or dense layer (see <a class="reference internal" href="#f-fully-connected"><span class="std std-numref">Fig. 5</span></a>).</p>
<p>In <a class="reference internal" href="#f-fully-connected"><span class="std std-numref">Fig. 5</span></a>, <em>N</em> is the total number of elements in the input and M
is the total number of neurons and is equal to output length. It
performs dot product operation of input tensor with each row of
weights matrix and adds biases to result:</p>
</div>
</div>
<div class="math notranslate nohighlight">
\[y_{i} = x_{j}W_{i,j} + b_{i}\]</div>
<blockquote>
<div><p>Where:</p>
<p><span class="math notranslate nohighlight">\(\ x_{j}\ \)</span> - <span class="math notranslate nohighlight">\(j_{\text{th}}\)</span> value in input tensor.</p>
<p><span class="math notranslate nohighlight">\(\ y_{i}\ \)</span> - <span class="math notranslate nohighlight">\(i_{\text{th}}\)</span> value in output tensor.</p>
<p><span class="math notranslate nohighlight">\(W_{\text{ij}}\ \)</span> - weight of <span class="math notranslate nohighlight">\(j_{\text{th}}\)</span> input
element for <span class="math notranslate nohighlight">\(i_{\text{th}}\)</span> neuron.</p>
<p><span class="math notranslate nohighlight">\(b_{j}\ \)</span> - bias for <span class="math notranslate nohighlight">\(i_{\text{th}}\)</span> neuron.</p>
<p>Ensure that the weight for this kernel is a two-dimensional tensor
(matrix) of shape [M, N], and Bias must be of shape [M]. Shape of
input tensor is not considered and only total number of elements is
considered. Kernel outputs a one-dimensional tensor of shape [M].</p>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Ensure that input and output
tensors do not point to
overlapped memory regions,
otherwise the behavior is
undefined.</p>
</div>
<div class="section" id="function-configuration-structure">
<span id="function-configuration-structure-4"></span><h3>Function Configuration Structure<a class="headerlink" href="#function-configuration-structure" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div>No configuration structure for fully connected kernel is required.
All necessary information is provided by tensors.</div></blockquote>
</div>
<div class="section" id="api">
<h3>API<a class="headerlink" href="#api" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Prototype</strong></td>
<td colspan="2"><div class="code c first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mli_status</span> <span class="n">mli_krn_fully_connected_</span>
<span class="o">&lt;</span><span class="n">data_type</span><span class="o">&gt;</span><span class="p">[</span><span class="n">_specialization</span><span class="p">](</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="ow">in</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">weights</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span>  <span class="o">*</span><span class="n">bias</span><span class="p">,</span>
   <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">out</span><span class="p">);</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>Parameters</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">in</span></code></td>
<td>[IN] Pointer to Input
feature map tensor</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">weights</span></code></td>
<td>[IN] Pointer to
weights tensor</td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">bias</span></code></td>
<td>[IN] Pointer to
biases tensor</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">out</span></code></td>
<td>[OUT] Pointer to
output tensor. Result
is stored here</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="kernel-specializations">
<h3>Kernel Specializations<a class="headerlink" href="#kernel-specializations" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="51%" />
<col width="49%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Function</strong></th>
<th class="head"><strong>Description</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_fully_connected_fx8</span></code></td>
<td>General function; 8bit FX
elements;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_fully_connected_fx16</span></code></td>
<td>General function; 16bit FX
elements;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_fully_connected_fx8w16d</span></code></td>
<td>General function; FX tensors
(8bit weights and biases, 16 bit
input and output);</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="conditions-for-applying-the-kernel">
<span id="conditions-apply-kernel"></span><h3>Conditions for Applying the Kernel<a class="headerlink" href="#conditions-for-applying-the-kernel" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Ensure that you satisfy the following conditions before applying the
function:</p>
<ul class="simple">
<li>Input, Weights and Bias tensors must be valid (see
<a class="reference internal" href="../mli_api_desc/general_considerations.html#mli-tns-struct"><span class="std std-ref">mli_tensor Structure</span></a>).</li>
<li>Weights must be two-dimensional tensor</li>
<li>Bias must be one-dimensional tensor. Its length must be equal to
number of neurons (first dimension of weights tensor)</li>
<li>Input tensor might be of any shape and rank</li>
<li>The second dimension of weight tensor must be equal to length of
input tensor (number of its elements)</li>
<li>Before processing, the output tensor does not have to contain valid
shape, rank, and element type fields. These are filled by the
function</li>
<li>Before processing, the output tensor must contain valid pointer to a
buffer with sufficient capacity (enough for result storing). It
also must contain a valid element parameter
(<code class="docutils literal notranslate"><span class="pre">el_params.fx.frac_bits</span></code>)</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="long-short-term-memory-lstm-cell">
<h2>Long Short Term Memory (LSTM) Cell<a class="headerlink" href="#long-short-term-memory-lstm-cell" title="Permalink to this headline">¶</a></h2>
<div class="figure" id="id10">
<span id="f-lstm"></span><img alt="../_images/image119.png" src="../_images/image119.png" />
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">Long Short Term Memory Schematic Representation</span></p>
<div class="legend">
<p>This kernel implements the default non-peephole implementation of
Long short term memory cell (see <a class="reference internal" href="#f-lstm"><span class="std std-numref">Fig. 6</span></a>).</p>
<p>The LSTM operation is described by the following formulas:</p>
</div>
</div>
<div class="math notranslate nohighlight">
\[{i_{t} = sigm\left( x_{t}W_{\text{xi}} + h_{t - 1}W_{\text{hi}} + b_{i} \right)}\]</div>
<div class="math notranslate nohighlight">
\[{f_{t} = sigm\left( x_{t}W_{\text{xf}} + h_{t - 1}W_{\text{hf}} + b_{f} \right)}\]</div>
<div class="math notranslate nohighlight">
\[{o_{t} = sigm\left( x_{t}W_{\text{xo}} + h_{t - 1}W_{\text{ho}} + b_{o} \right)}\]</div>
<div class="math notranslate nohighlight">
\[{g_{t} = \tanh\left( x_{t}W_{\text{xg}} + h_{t - 1}W_{\text{hg}} + b_{g} \right)}\]</div>
<div class="math notranslate nohighlight">
\[{C_{t} = g_{t}*i_{t} + f_{t}*C_{t - 1}}\]</div>
<div class="math notranslate nohighlight">
\[{h_{t} = o_{t}\ *o\_ act(C_{t})}\]</div>
<p>Where:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\ x_{t}\ \)</span> - frame <span class="math notranslate nohighlight">\(t\)</span> in input sequence.</p>
<p><span class="math notranslate nohighlight">\(\ h_{t}\ \)</span> - cell output for frame <span class="math notranslate nohighlight">\(t\)</span> in input
sequence.</p>
<p><span class="math notranslate nohighlight">\(i_{t}\ ,\ f_{t}\ ,\ o_{t}\)</span> – Input, forget, output gate
subtensors for frame <span class="math notranslate nohighlight">\(t\)</span> in input sequence.</p>
<p><span class="math notranslate nohighlight">\(\ g_{t}\ \)</span> - New cell candidates for frame <span class="math notranslate nohighlight">\(t\)</span> in input
sequence.</p>
<p><span class="math notranslate nohighlight">\(\ C_{t}\ \)</span> - Cell state for frame <span class="math notranslate nohighlight">\(t\)</span> in input sequence.</p>
<p><span class="math notranslate nohighlight">\(W_{**}\ \)</span> - weights for appropriate input subtensor.</p>
<p><span class="math notranslate nohighlight">\(b_{*}\ \)</span> - bias for appropriate input subtensor.</p>
<p><em>sigm</em> , <em>tanh</em> - sigmoid and hyperbolic tangent
activation functions.</p>
<p><span class="math notranslate nohighlight">\(o\_ act\)</span> – output activation function.</p>
<p>In <a class="reference internal" href="#f-lstm"><span class="std std-numref">Fig. 6</span></a>, <em>N</em> is the total number of elements in the input and M
is the total number of elements in the cell output.</p>
<p>Kernel supports various types of output activation (<span class="math notranslate nohighlight">\(o\_ act\)</span>
in the formula above):</p>
<ul class="simple">
<li><strong>Hyperbolic tangent</strong>: Uses TanH kernel of the library (see <a class="reference internal" href="transform_group.html#tanh"><span class="std std-ref">TanH</span></a>).
Number of fractional bits for output tensor is the same as that for
tensors processed by TanH activation.</li>
<li><strong>Sigmoid</strong>: Uses Sigmoid kernel of the library (see <a class="reference internal" href="transform_group.html#sigmoid"><span class="std std-ref">Sigmoid</span></a>). Number
of fractional bits for output tensor is the same as that for tensors
processed by Sigmoid activation.</li>
<li><strong>No Activation</strong>: Passes data without modification.</li>
</ul>
</div></blockquote>
<blockquote>
<div><p>The kernel takes 7 tensors including input, weights, cell,
intermediate tensor from configuration structure and others (for full
list, see <a class="reference internal" href="#api-lstm"><span class="std std-ref">API</span></a>). It modifies only output tensor, cell tensors, and
intermediate tensor in processing.</p>
<p>Weights for cell is a single three-dimensional tensor of shape [4, <em>M</em>,
<em>M+N</em>]. Ensure that bias is of shape [4, M]. It represents stacking
of all weights sub tensors into one tensor in order (I, g, f, o):</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
\begin{matrix}
W_{\text{xi}} \\
W_{\text{xg}} \\
\begin{matrix}
W_{\text{xf}} \\
W_{\text{xo}} \\
\end{matrix} \\
\end{matrix} &amp; \begin{matrix}
W_{\text{hi}} \\
W_{\text{hg}} \\
\begin{matrix}
W_{\text{hf}} \\
W_{\text{ho}} \\
\end{matrix} \\
\end{matrix} \\
\end{bmatrix}\text{ }\end{split}\]</div>
<blockquote>
<div>The first [M, <em>M+N]</em> sub-tensor of weights is applied to the input
gate, the second, tor new cell candidates, the third, to the forget
gate, and the last, to the output gate.</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>Ensure that you keep the same
order of sub-tensors for bias
tensor. For more information
about kernel parameters
requirements see <a class="reference internal" href="#cond-lstm"><span class="std std-ref">Conditions for Applying the Kernel</span></a>.</li>
<li>Ensure that the configuration
structure (see <a class="reference internal" href="#fn-conf-lstm"><span class="std std-ref">Function Configuration Structure</span></a>) also
contains the pointer to
tensor, which is used by
kernel as intermediate result
tensor. Kernel modifies the
memory pointed to by the data,
shape, rank, element type and
element parameters fields of
this tensor.</li>
<li>Ensure that the capacity of
the intermediate tensor is
enough to store M*4 elements
of input tensor type</li>
</ul>
</div>
<blockquote>
<div><p>Kernel supports three modes of input processing:</p>
<ul class="simple">
<li><strong>One-to-one</strong><ul>
<li>Processes the input tensor as a single input frame</li>
<li>Ignores the shape of input tensor, and considers only the total
number of elements</li>
<li>Performs single step to produce one-dimensional output tensor of
shape [<em>M</em>]</li>
<li>Updates the memory pointed to by cell tensor, but does not modify
the tensor’s fields</li>
</ul>
</li>
<li><strong>Batch-to-batch</strong><ul>
<li>Processes the input tensor as a sequence of frames to produce a
sequence of outputs of the same size</li>
<li>Considers first dimension of input tensor as sequence size
(<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>), and considers only the total number of elements
for the rest of the dimensions</li>
<li>Performs <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> steps to produce two-dimensional output tensor
of shape [<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, <em>M</em>]</li>
<li>Updates the memory pointed to by cell tensor, but does not modify
the tensor’s fields</li>
</ul>
</li>
<li><strong>Batch-to-last</strong><ul>
<li>Processes the input tensor as a sequence of frames to produce a
single (last in the sequence) output</li>
<li>Same as Batch-to-batch mode except that outputs tensor has a shape
[<em>M</em>] whose values are the same as those for the last sub
tensor in batch-to-batch mode</li>
</ul>
</li>
</ul>
</div></blockquote>
<blockquote>
<div>Dense part of calculations uses intermediate tensor for result, and
consequently output and previous output tensors might use the same
memory if it is acceptable to rewrite previous output data.</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Ensure that you allocate memory
for the rest of the tensors
(including intermediate results
tensor) without overlaps.
Otherwise the behavior is
undefined.</p>
</div>
<div class="section" id="fn-conf-lstm">
<span id="id1"></span><h3>Function Configuration Structure<a class="headerlink" href="#fn-conf-lstm" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Definition</strong></td>
<td colspan="2"><div class="code c first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">typedef</span> <span class="n">struct</span> <span class="p">{</span>
   <span class="n">mli_rnn_mode</span> <span class="n">mode</span><span class="p">;</span>
   <span class="n">mli_rnn_out_activation</span>  <span class="n">act</span><span class="p">;</span>
   <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">ir_tsr</span><span class="p">;</span>
 <span class="p">}</span> <span class="n">mli_rnn_cell_cfg</span><span class="p">;</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>Fields</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">mode</span></code></td>
<td>LSTM processing mode
(enumeration)</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">act</span></code></td>
<td>LSTM output
activation type
(enumeration)</td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">ir_tsr</span></code></td>
<td>Pointer to tensor for
holding intermediate
results. Tensor must
contain valid data
and capacity fields.
Field is modified by
kernels.</td>
</tr>
</tbody>
</table>
<p></p>
<span id="mli-rnn-mode-val-desc"></span><table border="1" class="colwidths-auto docutils" id="id11">
<caption><span class="caption-number">Table 13 </span><span class="caption-text">mli_rnn_mode Values Description</span><a class="headerlink" href="#id11" title="Permalink to this table">¶</a></caption>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Value</strong></th>
<th class="head"><strong>Field Description</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code></td>
<td>Process input tensor as a single
input frame .</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_BATCH</span></code></td>
<td>Process input tensor as a
sequence of frames to produce a
sequence of outputs .</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_LAST</span></code></td>
<td>Process input tensor as a
sequence of frames to produce
single (last) outputs.</td>
</tr>
</tbody>
</table>
<p></p>
<span id="mli-rnn-out-activation-val-desc"></span><table border="1" class="colwidths-auto docutils" id="id12">
<caption><span class="caption-number">Table 14 </span><span class="caption-text">mli_rnn_out_activation Values Description</span><a class="headerlink" href="#id12" title="Permalink to this table">¶</a></caption>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Value</strong></th>
<th class="head"><strong>Field Description</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">RNN_ACT_TANH</span></code></td>
<td>Hyperbolic tangent activation function.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">RNN_ACT_SIGM</span></code></td>
<td>Logistic (sigmoid) activation function.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">RNN_ACT_NONE</span></code></td>
<td>No activation.</td>
</tr>
</tbody>
</table>
<p></p>
</div>
<div class="section" id="api-lstm">
<span id="id2"></span><h3>API<a class="headerlink" href="#api-lstm" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Prototype</strong></td>
<td colspan="2"><div class="code c first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mli_status</span> <span class="n">mli_krn_lstm_cell_</span><span class="o">&lt;</span><span class="n">data_type</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">_specialization</span><span class="p">](</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="ow">in</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">prev_out</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">weights</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">bias</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_lstm_cell_cfg</span> <span class="o">*</span><span class="n">cfg</span><span class="p">,</span>
   <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">cell</span><span class="p">,</span>
   <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">out</span><span class="p">);</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>Parameters</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">in</span></code></td>
<td>[IN] Pointer to input
tensor</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">prev_out</span></code></td>
<td>[IN] Pointer to
previous output
tensor</td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">weights</span></code></td>
<td>[IN] Pointer to
weights tensor</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">bias</span></code></td>
<td>[IN] Pointer to
biases tensor</td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">cfg</span></code></td>
<td>[IN/OUT] Pointer to
configuration
structure</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">cell</span></code></td>
<td>[IN/OUT] Pointer to
cell state tensor</td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">out</span></code></td>
<td>[OUT] Pointer to
output tensor.</td>
</tr>
</tbody>
</table>
<p></p>
</div>
<div class="section" id="kernel-specializations-1">
<span id="id3"></span><h3>Kernel Specializations<a class="headerlink" href="#kernel-specializations-1" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Function</strong></th>
<th class="head"><strong>Description</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_lstm_cell_fx8</span></code></td>
<td>General function; 8bit FX
elements;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_lstm_cell_fx16</span></code></td>
<td>General function; 16bit FX
elements;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_lstm_cell_fx8w16d</span></code></td>
<td>General function; FX tensors
(8bit weights and biases, 16 bit
input, state, cell, output and
intermediate data);</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="cond-lstm">
<span id="id4"></span><h3>Conditions for Applying the Kernel<a class="headerlink" href="#cond-lstm" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Ensure that you satisfy the following conditions before applying the
function:</p>
<ul class="simple">
<li>Input, Weights, Bias, cell, and Previous output tensors must be valid
(see <a class="reference internal" href="../mli_api_desc/general_considerations.html#mli-tns-struct"><span class="std std-ref">mli_tensor Structure</span></a>)</li>
<li>Weights must be a three-dimensional tensor of shape [4, M, N+M]</li>
<li>Bias must be a two-dimensional tensor of shape [4, M]</li>
<li>Cell must be a one-dimensional tensor of shape [M]</li>
<li>Previous output must be a one-dimensional tensor of shape [M]</li>
<li>Element type of Weights and Bias tensors must be the same</li>
<li>Element type of Input, Cell and Previous output tensors must be the
same</li>
<li>The Input tensor has the following restrictions:<ul>
<li>For <code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code> mode, the total number of input and previous
output tensors (N+M) must be equal to the last dimension of
Weights tensor</li>
<li>For <code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_BATCH</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_LAST</span></code> modes, the first
dimension of input reflects sequence length (batch size) while for
the rest of the input tensor dimensions, the same rules apply as
for <code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code> mode</li>
</ul>
</li>
<li>The output tensor has the following restrictions:<ul>
<li>It must contain a valid pointer to a buffer with sufficient
capacity for storing the result (to keep M elements for
<code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_LAST</span></code> modes, and M*batch_size
elements for <code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_BATCH</span></code> mode)</li>
<li>If <code class="docutils literal notranslate"><span class="pre">RNN_ACT_NONE</span></code> is used as output activation, output tensor must
contain a valid element parameter (<code class="docutils literal notranslate"><span class="pre">el_params.fx.frac_bits</span></code>) and it
must be the same as for the previous output tensor</li>
<li>Before processing, the output tensor does not have to contain a
valid shape, rank, or element type. These are filled by function
according to inputs and kernel processing mode. If <code class="docutils literal notranslate"><span class="pre">RNN_ACT_NONE</span></code> is
not used, the same applies to element parameter
(<code class="docutils literal notranslate"><span class="pre">el_params.fx.frac_bits</span></code>)</li>
<li>Before processing, intermediate result tensor in config structure
must contain a valid pointer to a buffer with sufficient capacity
for the result (4*M elements of input type)</li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="basic-rnn-cell">
<h2>Basic RNN Cell<a class="headerlink" href="#basic-rnn-cell" title="Permalink to this headline">¶</a></h2>
<div class="figure" id="id13">
<span id="f-basic-rnn-cell"></span><img alt="../_images/image139.png" src="../_images/image139.png" />
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Basic RNN Cell Schematic Representation</span></p>
<div class="legend">
<p>This kernel implements the basic recurrent cell without memory state
(see <a class="reference internal" href="#f-basic-rnn-cell"><span class="std std-numref">Fig. 7</span></a>).</p>
<p>In <a class="reference internal" href="#f-basic-rnn-cell"><span class="std std-numref">Fig. 7</span></a>, <em>N</em> is the total number of elements in the input and M
is the total number of elements in the cell output.</p>
<p>Basic RNN operation is described by the following formula:</p>
</div>
</div>
<div class="math notranslate nohighlight">
\[h_{t} = f(x_{t}W_{x} + h_{t - 1}W_{h} + b)\]</div>
<blockquote>
<div><p>Where:</p>
<p><span class="math notranslate nohighlight">\(\ x_{t}\ \)</span> - frame <span class="math notranslate nohighlight">\(t\)</span> in input sequence.</p>
<p><span class="math notranslate nohighlight">\(\ h_{t}\ \)</span> - cell output for frame <span class="math notranslate nohighlight">\(t\)</span> in input
sequence.</p>
<p><span class="math notranslate nohighlight">\(W_{*}\ \)</span> - weights for appropriate input subtensor.</p>
<p><span class="math notranslate nohighlight">\(b_{*}\ \)</span> - bias for appropriate input subtensor.</p>
<p><span class="math notranslate nohighlight">\(f()\)</span> - output activation function.</p>
<p>Kernel supports following types of output activation (<span class="math notranslate nohighlight">\(f()\)</span> in
the formula above) :</p>
<ul class="simple">
<li>Hyperbolic tangent. Uses TanH kernel of the library (see <a class="reference internal" href="transform_group.html#tanh"><span class="std std-ref">TanH</span></a>).</li>
<li>Sigmoid. Uses Sigmoid kernel of the library (see <a class="reference internal" href="transform_group.html#sigmoid"><span class="std std-ref">Sigmoid</span></a>)</li>
<li>No Activation. Passes data without modification</li>
</ul>
</div></blockquote>
<blockquote>
<div><p>Kernel modifies only output tensors and intermediate tensor from
configuration structure in processing. For a full list of parameters
see <a class="reference internal" href="#api-brnn"><span class="std std-ref">API</span></a>.</p>
<p>Kernel supports three modes of input processing</p>
<ul class="simple">
<li><strong>One-to-one</strong><ul>
<li>Processes the input tensor as a single input frame</li>
<li>Ignores the shape of input tensor, and only considers the total
number of elements</li>
<li>Performs single step to produce a one-dimensional output tensor of
shape [M]</li>
</ul>
</li>
<li><strong>Batch-to-batch</strong><ul>
<li>Processes the input tensor as a sequence of frames to produce a
sequence of outputs of the same size</li>
<li>Considers the first dimension of input tensor as sequence size
(<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>), and considers the total number of elements for the
rest of the dimensions.</li>
<li>Performs <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> steps to produce 2 dimensional output tensor
of shape [<code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, M].</li>
</ul>
</li>
<li><strong>Batch-to-last</strong><ul>
<li>Processes the input tensor as a sequence of frames to produce a
single (last in the sequence) output</li>
<li>Same as batch-to-batch mode except that outputs tensor has a shape
[M] whose values are the same as those for the last sub tensor in
batch-to-batch mode</li>
</ul>
</li>
</ul>
</div></blockquote>
<blockquote>
<div>Weights for a cell is a single 2-dimensionl tensor of shape [<em>M</em>,
<em>M+N</em>], an Bias is of shape [M]. It represents the stacking of 2
weights sub-tensors into one tensor in the following order:</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
W_{x} &amp; W_{h} \\
\end{bmatrix}\text{ }\end{split}\]</div>
<blockquote>
<div><p>To support user-specific complex recurrent cells beside LSTM, basic
RNN cell kernel in One-to-One mode can work with matrixes with
stacked weights to produce stacked output tensor.</p>
<p>For example, if weights tensor is 3-dimensionl tensor of shape [<em>L</em>,
<em>M</em>, <em>M+N</em>], and Bias of shape [<em>L, M</em>], the output tensor is of
shape [<em>L</em>, <em>M</em>].</p>
<p>In batch-to-last mode, configuration structure also contains pointer
to the tensor that is used by kernel as intermediate result tensor.
Kernel modifies the memory pointed to by data, shape, rank, element
type and element parameters fields of this tensor. Ensure that the
capacity of the intermediate tensor is enough to store the output for
one step of kernel (M or L*M elements for stacked weights matrix).</p>
<p>For the other modes (one-to-one or batch-to-batch) kernel does not
use the intermediate result tensor and this field might not be
initialized. For more information about configuration structure see
<a class="reference internal" href="#fn-conf-lstm"><span class="std std-ref">Function Configuration Structure</span></a>.</p>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Ensure that you allocate memory
for all tensors (including
intermediate results tensor)
without overlaps.</p>
<p class="last">The only exception is
batch-to-last mode due to its
usage of intermediate tensor. In
this case, the output and the previous
output tensors might use the same
memory if it is acceptable to
rewrite previous output data.</p>
</div>
<div class="section" id="fn-conf-brnn">
<span id="id5"></span><h3>Function Configuration Structure<a class="headerlink" href="#fn-conf-brnn" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div>Basic RNN cell kernel shares configuration structure with LSTM cell.
For more information see <a class="reference internal" href="#fn-conf-lstm"><span class="std std-ref">Function Configuration Structure</span></a>.</div></blockquote>
</div>
<div class="section" id="api-brnn">
<span id="id6"></span><h3>API<a class="headerlink" href="#api-brnn" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Prototype</strong></td>
<td colspan="2"><div class="code c first last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mli_status</span> <span class="n">mli_krn_basic_rnn_cell_</span><span class="o">&lt;</span><span class="n">data_type</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">_specialization</span><span class="p">](</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="ow">in</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">prev_out</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">weights</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">bias</span><span class="p">,</span>
   <span class="n">const</span> <span class="n">mli_rnn_cell_cfg</span> <span class="o">*</span><span class="n">cfg</span><span class="p">,</span>
   <span class="n">mli_tensor</span> <span class="o">*</span><span class="n">out</span><span class="p">);</span>
</pre></div>
</div>
</td>
</tr>
<tr class="row-even"><td><strong>Parameters</strong></td>
<td><code class="docutils literal notranslate"><span class="pre">in</span></code></td>
<td>[IN] Pointer to input
tensor</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">prev_out</span></code></td>
<td>[IN] Pointer to
previous output
tensor</td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">weights</span></code></td>
<td>[IN] Pointer to
weights tensor</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">bias</span></code></td>
<td>[IN] Pointer to
biases tensor</td>
</tr>
<tr class="row-even"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">cfg</span></code></td>
<td>[IN/OUT] Pointer to
configuration
structure</td>
</tr>
<tr class="row-odd"><td>&#160;</td>
<td><code class="docutils literal notranslate"><span class="pre">out</span></code></td>
<td>[OUT] Pointer to
output tensor. Result
is stored here</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="kernel-specializations-2">
<span id="id7"></span><h3>Kernel Specializations<a class="headerlink" href="#kernel-specializations-2" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="53%" />
<col width="47%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Function</strong></th>
<th class="head"><strong>Description</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_basic_rnn_cell_fx8</span></code></td>
<td>General function; 8bit FX
elements;</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_basic_rnn_cell_fx16</span></code></td>
<td>General function; 16bit FX
elements;</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">mli_krn_basic_rnn_cell_fx8w16d</span></code></td>
<td>General function; FX tensors
(8bit weights and biases, 16 bit
input, state, cell, output and
intermediate data);</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="conditions-for-applying-the-kernel-2">
<span id="id8"></span><h3>Conditions for Applying the Kernel<a class="headerlink" href="#conditions-for-applying-the-kernel-2" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p>Ensure that you satisfy the following conditions before applying the
function:</p>
<ul class="simple">
<li>Input, Weights, Bias, and Previous output tensors must be valid (see
<a class="reference internal" href="../mli_api_desc/general_considerations.html#mli-tns-struct"><span class="std std-ref">mli_tensor Structure</span></a>).</li>
<li>Weights is a two-dimensional tensor of shape [M, N+M]. But In
<code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code> mode, the weights tensor is of shape [L, M, N+M] to
produce an output tensor of shape [L, M].</li>
<li>Bias is a one-dimensional tensor of shape [M]. But In <code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code>
mode, bias tensor is of shape [L, M] to produce an output tensor
of shape [L, M].</li>
<li>Previous output must be a one-dimensional tensor of shape [M]</li>
<li>Element type of Weights and Bias tensors must be the same.</li>
<li>Element type of Input, Previous output tensors must be the same.</li>
<li>The input tensor has the following restrictions:<ul>
<li>For <code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code> mode, the total number of input and previous
output tensors (N+M) must be equal to the last dimension of
Weights tensor.</li>
<li>For <code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_BATCH</span></code> and <code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_LAST</span></code> modes, first
dimension of input reflects sequence length (batch size) while for
the rest of the input tensor dimensions the same rules apply as
those for the <code class="docutils literal notranslate"><span class="pre">RNN_ONE_TO_ONE</span></code> mode.</li>
</ul>
</li>
<li>The output tensor has the following restrictions:<ul>
<li>It must contain a valid pointer to a buffer with sufficient
capacity for storing the result (to keep <em>M</em> or <em>L*M</em> elements for
RNN_ONE_TO_ONE and RNN_BATCH_TO_LAST modes, and <em>M</em>*batch_size
elements for RNN_BATCH_TO_BATCH mode)</li>
<li>If <code class="docutils literal notranslate"><span class="pre">RNN_ACT_NONE</span></code> is used as output activation, output tensor must
contain a valid element parameter (el_params.fx.frac_bits) and it
must be the same as that for the previous output tensor.</li>
<li>Before processing, the output tensor does not have to contain a
valid shape, rank and element type. These are filled by function
according to inputs, and kernel processing mode. If RNN_ACT_NONE
is not used, the same rule applies for element parameter
(<code class="docutils literal notranslate"><span class="pre">el_params.fx.frac_bits</span></code>).</li>
</ul>
</li>
<li>The intermediate result tensor in config structure has the following
restrictions:<ul>
<li>For <code class="docutils literal notranslate"><span class="pre">RNN_BATCH_TO_LAST</span></code> mode, it must contain a valid pointer to a
buffer with sufficient capacity for storing the result (M elements
of input type).</li>
<li>In other cases, this tensor is not used and might be used to hold
any data.</li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="transform_group.html" class="btn btn-neutral float-right" title="Transform Group" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="pooling_group.html" class="btn btn-neutral float-left" title="Pooling Group" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Synopsys, Inc

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>